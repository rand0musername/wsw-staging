<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="Watermark Stealing in Large Language Models"/>
  <meta property="og:description" content="Prominent LLM watermarks can be stolen for under $50, enabling spoofing and scrubbing attacks at scale."/>
  <meta property="og:url" content="https://watermark-stealing.org"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/banner-og.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Watermark Stealing in Large Language Models">
  <meta name="twitter:description" content="Prominent LLM watermarks can be stolen for under $50, enabling spoofing and scrubbing attacks at scale.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/banner-twitter.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="large language models, watermarks, watermarking, llm, safety, security, attacks, spoofing, scrubbing">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Watermark Stealing</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico?v=2">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>




  <section class="hero is-purple">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h2 class="title publication-title"><img class="logo" src="static/images/logo.png"> Watermark Stealing in Large Language Models</h2>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://www.sri.inf.ethz.ch/people/nikola" target="_blank">Nikola JovanoviÄ‡</a>,</span>
              <span class="author-block">
                  <a href="https://www.sri.inf.ethz.ch/people/robin" target="_blank">Robin Staab</a>,</span>
              <span class="author-block">
                  <a href="https://www.sri.inf.ethz.ch/people/martin" target="_blank">Martin Vechev</a></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">ETH Zurich</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://files.sri.inf.ethz.ch/website/papers/jovanovic2024watermarkstealing.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                  </a>
                </span>
        
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2402.19361" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                  </a>
                </span>
              
                <span class="link-block">
                  <a href="https://github.com/eth-sri/watermark-stealing" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://twitter.com/ni_jovanovic" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-twitter"></i>
                  </span>
                  <span>Summary</span>
                  </a>
                </span>
            </div> <!-- publications links -->
          </div> <!-- column for publications links -->


        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered tldr">
            <b>TL;DR</b>:
            Prominent LLM watermarks can be stolen for under $50, enabling spoofing and scrubbing attacks at scale.
      </div>
    </div>
  </div>
</div>
</section>


<!-- Paper overview -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Stolen?</h2>
        <img class="figure" src="static/images/stealing.png" alt="Stealing" class="">
        <div class="content has-text-justified">
          <br>
          <p>
            <ul>
            <li> This is placeholder text.
            <li> LLM watermarking has attracted attention as a promising way to detect AI-generated content, with some works suggesting that current schemes may already be fit for deployment.
            <li> In this work we dispute this claim, identifying <b>watermark stealing</b> (WS) as a fundamental vulnerability of these schemes.
            <li>We show that querying the API of the watermarked LLM to approximately reverse-engineer a watermark enables practical <b>spoofing attacks</b>, as suggested in prior work, but also greatly boosts <b>scrubbing attacks</b>, which was previously unnoticed.
            <li>We are the first to propose an automated WS algorithm and use it in the first comprehensive study of spoofing and scrubbing in realistic settings.
          </ul>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Spoofing attacks?</h2>
          <img class="figure" src="static/images/spoofing.png" alt="Spoofing" class="">
          <div class="content has-text-centered has-text-justified">
          <br>
          <p>
            <ul>
            <li> This is placeholder text.
            <li> LLM watermarking has attracted attention as a promising way to detect AI-generated content, with some works suggesting that current schemes may already be fit for deployment.
            <li> In this work we dispute this claim, identifying <b>watermark stealing</b> (WS) as a fundamental vulnerability of these schemes.
            <li>We show that querying the API of the watermarked LLM to approximately reverse-engineer a watermark enables practical <b>spoofing attacks</b>, as suggested in prior work, but also greatly boosts <b>scrubbing attacks</b>, which was previously unnoticed.
            <li>We are the first to propose an automated WS algorithm and use it in the first comprehensive study of spoofing and scrubbing in realistic settings.
          </ul>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Scrubbing attacks?</h2>
          <img class="figure" src="static/images/scrubbing.png" alt="Scrubbing" class="">
          <div class="content has-text-centered has-text-justified">
          <br>
          <p>
            LLM watermarking has attracted attention as a promising way to detect AI-generated content, with some works suggesting that current schemes may already be fit for deployment.
            In this work we dispute this claim, identifying <b>watermark stealing</b> (WS) as a fundamental vulnerability of these schemes.
            We show that querying the API of the watermarked LLM to approximately reverse-engineer a watermark enables practical <b>spoofing attacks</b>, as suggested in prior work, but also greatly boosts <b>scrubbing attacks</b>, which was previously unnoticed.
            We are the first to propose an automated WS algorithm and use it in the first comprehensive study of spoofing and scrubbing in realistic settings.
            We show that for <b>under $50 an attacker can both spoof and scrub state-of-the-art schemes</b> previously considered safe, with average success rate of over 80%.
            Our findings challenge common beliefs about LLM watermarking, stressing the need for more robust schemes.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Examples</h2>
          <div class="content has-text-centered has-text-justified">
          <br>
          <p>
            Use the toggle to toggle the detector view. The detector will show the watermark of the LLM and the confidence of the detection.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">What now?</h2>
          <div class="content has-text-centered has-text-justified">
          <br>
          <p>
            Use the toggle to toggle the detector view. The detector will show the watermark of the LLM and the confidence of the detection.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper overview -->


<!--
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            LLM watermarking has attracted attention as a promising way to detect AI-generated content, with some works suggesting that current schemes may already be fit for deployment.
            In this work we dispute this claim, identifying <b>watermark stealing</b> (WS) as a fundamental vulnerability of these schemes.
            We show that querying the API of the watermarked LLM to approximately reverse-engineer a watermark enables practical <b>spoofing attacks</b>, as suggested in prior work, but also greatly boosts <b>scrubbing attacks</b>, which was previously unnoticed.
            We are the first to propose an automated WS algorithm and use it in the first comprehensive study of spoofing and scrubbing in realistic settings.
            We show that for <b>under $50 an attacker can both spoof and scrub state-of-the-art schemes</b> previously considered safe, with average success rate of over 80%.
            Our findings challenge common beliefs about LLM watermarking, stressing the need for more robust schemes.
          </p>
        </div>
      </div>
    </div>
  </div>
</section> -->

<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">Cite our work:</h2>
      <pre><code>@article{jovanovic2024watermarkstealing,
  title = {Watermark Stealing in Large Language Models},
  author = {JovanoviÄ‡, Nikola and Staab, Robin and Vechev, Martin},
  year = {2024},
  eprint={2402.19361},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}</code></pre>
    </div>
</section>


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content has-text-centered">
            Website and project are part of the <b><a href="https://sri.inf.ethz.ch">Secure, Reliable and Intelligent Systems Lab at ETH Zurich</a></b>.
            <br>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
            <br>
            <br>
            <img class="logos" src="static/images/footer.svg" alt="ETH & SRI Logo">
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
